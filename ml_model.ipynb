{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_data = []\n",
    "relative_path = os.path.join(os.getcwd(), 'data', 'AdienceBenchmarkGenderAndAgeClassification')\n",
    "\n",
    "for file in os.listdir(relative_path):\n",
    "    if file.endswith('.txt'):\n",
    "       txt_data.append(pd.read_csv(os.path.join(relative_path, file), sep='\\t'))\n",
    "\n",
    "raw_data = pd.concat(txt_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a absolute path to dataset which contains imges for a model\n",
    "raw_data.loc[:, 'img_folder_path'] = raw_data.apply(lambda row: os.path.join(relative_path, 'faces', row.user_id), axis=1)\n",
    "raw_data.loc[:, 'img_name'] = raw_data.apply(lambda row: 'coarse_tilt_aligned_face.' + str(row.face_id) + '.' + row.original_image, axis=1)\n",
    "raw_data.loc[:, 'img_absolute_path'] = raw_data.apply(lambda row: os.path.join(row.img_folder_path, row.img_name), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = 'coarse_tilt_aligned_face.' + str(raw_data.face_id.loc[5]) + '.' + raw_data.original_image.loc[5]\n",
    "img_path = os.path.join(relative_path, 'faces', raw_data.user_id.iloc[5], img_name)\n",
    "img = load_img(img_path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check if data contains a missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                 0\n",
       "original_image          0\n",
       "face_id                 0\n",
       "age                     0\n",
       "gender                779\n",
       "x                       0\n",
       "y                       0\n",
       "dx                      0\n",
       "dy                      0\n",
       "tilt_ang                0\n",
       "fiducial_yaw_angle      0\n",
       "fiducial_score          0\n",
       "img_folder_path         0\n",
       "img_name                0\n",
       "img_absolute_path       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NA data from gender column\n",
    "raw_data.dropna(subset=['gender'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f    9372\n",
       "m    8120\n",
       "u    1099\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f = female, m=male, u=unidetifie \n",
    "raw_data.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop 'u' data from gender which stays as unindetyfie\n",
    "raw_data = raw_data[raw_data.gender != 'u']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['(25, 32)', '(38, 43)', '(4, 6)', '(60, 100)', '(15, 20)',\n",
       "       '(48, 53)', '(8, 12)', '(0, 2)', '(38, 48)', '35', '3', '55', '58',\n",
       "       '22', '13', '45', '36', '23', '(38, 42)', 'None', '(8, 23)',\n",
       "       '(27, 32)', '57', '2', '29', '34', '42', '46'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at second column to predict - > age\n",
    "raw_data.age.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop data where age is unkonow\n",
    "raw_data = raw_data[raw_data.age != 'None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17452 entries, 0 to 19345\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   user_id             17452 non-null  object\n",
      " 1   original_image      17452 non-null  object\n",
      " 2   face_id             17452 non-null  int64 \n",
      " 3   age                 17452 non-null  object\n",
      " 4   gender              17452 non-null  object\n",
      " 5   x                   17452 non-null  int64 \n",
      " 6   y                   17452 non-null  int64 \n",
      " 7   dx                  17452 non-null  int64 \n",
      " 8   dy                  17452 non-null  int64 \n",
      " 9   tilt_ang            17452 non-null  int64 \n",
      " 10  fiducial_yaw_angle  17452 non-null  int64 \n",
      " 11  fiducial_score      17452 non-null  int64 \n",
      " 12  img_folder_path     17452 non-null  object\n",
      " 13  img_name            17452 non-null  object\n",
      " 14  img_absolute_path   17452 non-null  object\n",
      "dtypes: int64(8), object(7)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select data which will be needed to train and test models\n",
    "data = raw_data[['age', 'gender', 'x', 'y', 'dx', 'dy', 'img_absolute_path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode gender categorical data to numeric \n",
    "data = data.replace({'gender': {'f': 0, 'm': 1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['(25, 32)', '(38, 43)', '(4, 6)', '(60, 100)', '(15, 20)',\n",
       "       '(48, 53)', '(8, 12)', '(0, 2)', '(38, 48)', '35', '3', '55', '58',\n",
       "       '22', '13', '45', '36', '23', '(38, 42)', '(8, 23)', '(27, 32)',\n",
       "       '57', '2', '29', '34', '42', '46'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode age values\n",
    "data.age.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mapping= [('(0, 2)', '0-2'), ('2', '0-2'), ('3', '0-2'), ('(4, 6)', '4-6'), ('(8, 12)', '8-13'), ('13', '8-13'), ('22', '15-20'), ('(8, 23)','15-20'), ('23', '25-32'), \n",
    "('(15, 20)', '15-20'), ('(25, 32)', '25-32'), ('(27, 32)', '25-32'), ('32', '25-32'), ('34', '25-32'), ('29', '25-32'), ('(38, 42)', '38-43'), ('35', '38-43'), ('36', '38-43'), \n",
    "('42', '48-53'), ('45', '38-43'), ('(38, 43)', '38-43'), ('(38, 42)', '38-43'), ('(38, 48)', '48-53'), ('46', '48-53'), ('(48, 53)', '48-53'), ('55', '48-53'), ('56', '48-53'), \n",
    "('(60, 100)', '60+'), ('57', '60+'), ('58', '60+')]\n",
    "\n",
    "age_mapping_dict = {x[0]: x[1] for x in age_mapping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode categorical age data to numeric\n",
    "age_dict =  {'0-2': 0, '4-6': 1, '8-13': 2, '15-20': 3, '25-32': 4, '38-43': 5, '48-53': 6, '60+': 7}\n",
    "data.loc[:, 'age'] = data.apply(lambda row: age_dict[age_mapping_dict[row.age]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17452 entries, 0 to 19345\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   age                17452 non-null  int64 \n",
      " 1   gender             17452 non-null  int64 \n",
      " 2   x                  17452 non-null  int64 \n",
      " 3   y                  17452 non-null  int64 \n",
      " 4   dx                 17452 non-null  int64 \n",
      " 5   dy                 17452 non-null  int64 \n",
      " 6   img_absolute_path  17452 non-null  object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data on train_test values, in our case we split the absolute path to folders, where we store ours images. After this process we will preproces imgaes for a deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_gender = data['img_absolute_path']\n",
    "y_gender = data['gender']\n",
    "\n",
    "\n",
    "\n",
    "X_train_gender, X_test_gender, y_train_gedner, y_test_gender = train_test_split(X_gender, y_gender, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images Preprocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opis dlaczego w taki sposób podchodzimy do danych wsadowych. Powód rozmieszenie w bazie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, load_img, smart_resize\n",
    "\n",
    "def resize_array_img(img_path): \n",
    "    image_array = img_to_array(smart_resize(load_img(img_path), (227, 227)))\n",
    "    return image_array\n",
    "\n",
    "train_images = []\n",
    "test_images = []\n",
    "\n",
    "for img_path in X_train_gender:\n",
    "    img_data = resize_array_img(img_path=img_path)\n",
    "    train_images.append(img_data)\n",
    "    break\n",
    "\n",
    "for img_path in X_test_gender:\n",
    "    img_data = resize_array_img(img_path=img_path)\n",
    "    test_images.append(img_data)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja do tworzenia modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, LayerNormalization\n",
    "\n",
    "def create_gender_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape=(227, 227, 3), filters=96, kernel_size=(7, 7), strides=4, padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units=512, activation='relu'))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Dense(units=512, activation='relu'))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stworzenie modelu oraz podsumowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 56, 56, 96)        14208     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 28, 28, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 28, 28, 96)       192       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 14, 14, 256)      512       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 7, 7, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " layer_normalization_2 (Laye  (None, 7, 7, 256)        512       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               6423040   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,906,882\n",
      "Trainable params: 7,906,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gender_model = create_gender_model()\n",
    "gender_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=\"adam\", \n",
    "              metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model.fit(train_images, y_train_gedner,\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34e7097143a4bbeb644b1636d7c488b335524c25918bb77f349162bb29d7a115"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('gda_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
